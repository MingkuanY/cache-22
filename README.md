# Cache 22

We can cache GPT responses to lower the necessary compute. When the user prompts the model, the prompt is first broken down into components. A similarity score is calculated for each component that determines whether the question has already been asked before in its cache. If it has, then the algorithm simply retrieves the answer from the cache and appends it to the generated response. If not, then it will use normal compute to generate it. After all responses to each component of the question is appended together, it runs it through a synthesis model which uses very little compute which is sent to the user. By not generating the response from scratch every time, this algorithm seeks to vastly reduce the compute needed to query transformers, which can lead to faster queries and lower energy costs.
